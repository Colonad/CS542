# ==============================================================================
# Real Estate Price Prediction â€” Master Config
# ==============================================================================

# ---------------------------- #
# Paths & run configuration
# ---------------------------- #
paths:
  data_csv: data/raw/usa_real_estate.csv
  interim_dir: data/interim
  out_dir: outputs

run:
  run_name: null                  # if null, auto: YYYY-MM-DD_HHMMSS
  timestamped_outputs: true       # if true, mirror metrics/preds/models into outputs/run/<stamp>/
  save_config_snapshot: true      # copy this yaml into the run folder
  random_state: 0
  n_jobs: -1                      # threads for models that support it
  float_precision: 6

# ---------------------------- #
# Data loading & sanitation
# ---------------------------- #
data:
  required_columns: ["price", "sold_date"]
  parse_dates: ["sold_date", "prev_sold_date"]
  drop_inf: true
  drop_na_target: true
  # broad guards (do not overfit here; just remove clearly impossible values)
  filters:
    price:
      min: 1000
      max: 10000000
    house_size:
      min: 100           # optional; only applied if column exists
      max_percentile: 99.9
    acre_lot:
      min: 0
      max_percentile: 99.9
  export_schema_snapshot: true     # write schema snapshot to data/interim/schema.snapshot.json

# ---------------------------- #
# Target & transformation
# ---------------------------- #
target:
  name: price
  log_transform: true             # model log1p(price); inverse with expm1 for reporting

# ---------------------------- #
# Feature configuration
# ---------------------------- #
features:
  # Base feature lists (only columns present in data will be used)
  numeric: [bed, bath, acre_lot, house_size, years_since_prev_sale, year, month]
  categorical: [city, state, zip_code]

  # Automatically derived (no leakage; derived from a single row)
  derived:
    enable_basic_time: true       # adds year, month
    enable_years_since_prev: true # (sold - prev_sold)/365.25; impute if prev missing

  # Optional engineered features computed row-wise
  engineered:
    - name: bed_per_bath
      expr: "bed / np.clip(bath, 1, None)"
    - name: lot_per_size
      expr: "acre_lot / np.clip(house_size, 1, None)"
    - name: log_house_size
      expr: "np.log1p(house_size)"

  # Neighborhood/aggregate stats fit on TRAIN ONLY (then merged to val/test)
  leakage_safe_neighbors:
    enabled: true
    group_keys: [zip_code, year]  # must be in dataframe
    stats:
      - kind: median
        of: price
        name: zip_year_price_median
    min_group_size: 50
    fill_strategy: global_median   # fallback for unseen groups

  # Outlier handling (simple winsorization for selected numeric columns)
  winsorize:
    enabled: true
    columns: [house_size, acre_lot, bed, bath]
    lower_pct: 0.0
    upper_pct: 99.5

# ---------------------------- #
# Split protocol
# ---------------------------- #
split:
  # 2-way temporal split (train < cutoff, test >= cutoff)
  cutoff: "2023-01-01"
  # Optional 3-way split: if provided, val = [cutoff, val_cutoff), test >= val_cutoff
  val_cutoff: null
  # If a stable property ID exists, you can forbid property leakage across splits
  property_id_col: null           # e.g., "prop_id" (if you build one)

# ---------------------------- #
# Cross-validation (optional)
# ---------------------------- #
cv:
  enabled: false
  kind: time_series               # 'time_series' | 'kfold'
  n_splits: 3
  gap: 0                           # gap between train and val windows (in months) for time series
  test_size_months: 6

# ---------------------------- #
# Baselines
# ---------------------------- #
baselines:
  median_zip_year:
    enabled: true
    group_cols: [zip_code, year]

# ---------------------------- #
# Preprocessing
# ---------------------------- #
preprocess:
  numeric_imputer: median         # 'median' | 'mean' | 'constant'
  categorical_imputer: most_frequent
  one_hot:
    handle_unknown: ignore
    min_frequency: 50             # groups rare categories to keep matrix manageable
  scaler: null                    # (not needed for tree models; set to 'standard' for linear if desired)

# ---------------------------- #
# Model registry & params
# ---------------------------- #
model:
  kind: random_forest             # ridge | random_forest | xgboost | lightgbm (if installed)

  ridge:
    alpha: 2.0
    fit_intercept: true
    copy_X: true
    random_state: 0

  random_forest:
    n_estimators: 600
    max_depth: null
    min_samples_split: 2
    min_samples_leaf: 2
    max_features: "sqrt"
    bootstrap: true
    n_jobs: -1
    random_state: 0


  xgboost:
    enabled: true
    n_estimators: 1200
    max_depth: 8
    learning_rate: 0.05
    subsample: 0.8
    colsample_bytree: 0.8
    reg_lambda: 1.0
    reg_alpha: 0.0
    tree_method: auto
    n_jobs: -1
    random_state: 0
    early_stopping:
      enabled: false              # turn on if you implement 3-way split with validation

  lightgbm:
    enabled: false                # set true if you add LightGBM to requirements
    n_estimators: 1200
    num_leaves: 127
    learning_rate: 0.05
    subsample: 0.8
    colsample_bytree: 0.8
    reg_lambda: 1.0
    random_state: 0

# ---------------------------- #
# Training run behavior
# ---------------------------- #
train:
  save_pipeline: true
  save_feature_importances: true
  model_filename: price_model.pkl

# ---------------------------- #
# Evaluation & reporting
# ---------------------------- #
eval:
  metrics: [MAE, RMSE, R2]
  report_currency: USD

  # Diagnostic slices (compute MAE per group)
  slices:
    enabled: true
    group_cols: [state, zip_code]
    topk: 20

  # Plots saved to outputs/figures/
  plots:
    pred_vs_actual: true
    residuals_vs_price: true
    feature_importance: true
    calibration_curve: false

  # Export files
  exports:
    metrics_json: true
    summary_csv: true
    preds_csv: true
    slices_json: true

# ---------------------------- #
# Interpretability 
# ---------------------------- #
interpretability:
  permutation_importance:
    enabled: true
    n_repeats: 5
    subsample_rows: 20000
  shap:
    enabled: false                # enable only if you install shap; can be slow
    sample_rows: 5000

# ---------------------------- #
# Logging
# ---------------------------- #
logging:
  level: INFO
  log_to_file: false
  file: null

# ---------------------------- #
# Reproducibility/metadata
# ---------------------------- #
repro:
  capture_git_commit: true        # if repo is a git repo, include HEAD sha in metrics.json
  capture_data_hash: true         # sha256 of raw CSV
